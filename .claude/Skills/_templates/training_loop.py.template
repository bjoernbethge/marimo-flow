"""
Generic Training Loop Template

This template provides a flexible training loop structure that can be adapted
for various ML frameworks (PyTorch, PINA, Trainer-based, etc.)

Usage:
1. Replace placeholders with your specific model/data
2. Customize training parameters
3. Add framework-specific callbacks/metrics
"""

import torch
from typing import Optional, Dict, Any, Callable
from pathlib import Path


class GenericTrainer:
    """
    Generic training loop that works with various frameworks.

    Customize for your specific use case:
    - PyTorch: Implement train_step, val_step
    - PINA: Use pina.Trainer directly
    - MLflow: Add mlflow logging
    """

    def __init__(
        self,
        model: Any,
        train_loader: Any,
        val_loader: Optional[Any] = None,
        optimizer: Optional[Any] = None,
        loss_fn: Optional[Callable] = None,
        device: str = "cpu",
        max_epochs: int = 100,
        checkpoint_dir: Optional[Path] = None,
    ):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        self.device = device
        self.max_epochs = max_epochs
        self.checkpoint_dir = checkpoint_dir

        # Training state
        self.current_epoch = 0
        self.history = {"train_loss": [], "val_loss": []}

        # Move model to device
        if hasattr(model, "to"):
            self.model = model.to(device)

    def train_step(self, batch: Any) -> float:
        """
        Execute one training step.

        Override this method for custom training logic.
        """
        self.model.train()
        self.optimizer.zero_grad()

        # Extract inputs/targets from batch
        # Customize based on your data format
        inputs, targets = batch
        inputs = inputs.to(self.device)
        targets = targets.to(self.device)

        # Forward pass
        outputs = self.model(inputs)
        loss = self.loss_fn(outputs, targets)

        # Backward pass
        loss.backward()
        self.optimizer.step()

        return loss.item()

    def val_step(self, batch: Any) -> float:
        """
        Execute one validation step.

        Override this method for custom validation logic.
        """
        self.model.eval()

        with torch.no_grad():
            inputs, targets = batch
            inputs = inputs.to(self.device)
            targets = targets.to(self.device)

            outputs = self.model(inputs)
            loss = self.loss_fn(outputs, targets)

        return loss.item()

    def train_epoch(self) -> float:
        """Train for one epoch."""
        total_loss = 0.0
        num_batches = 0

        for batch in self.train_loader:
            loss = self.train_step(batch)
            total_loss += loss
            num_batches += 1

        return total_loss / num_batches if num_batches > 0 else 0.0

    def validate(self) -> Optional[float]:
        """Run validation."""
        if self.val_loader is None:
            return None

        total_loss = 0.0
        num_batches = 0

        for batch in self.val_loader:
            loss = self.val_step(batch)
            total_loss += loss
            num_batches += 1

        return total_loss / num_batches if num_batches > 0 else 0.0

    def save_checkpoint(self, filename: str = "checkpoint.pt"):
        """Save model checkpoint."""
        if self.checkpoint_dir is None:
            return

        checkpoint_path = self.checkpoint_dir / filename
        checkpoint_path.parent.mkdir(parents=True, exist_ok=True)

        checkpoint = {
            "epoch": self.current_epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "history": self.history,
        }

        torch.save(checkpoint, checkpoint_path)
        print(f"Checkpoint saved to {checkpoint_path}")

    def load_checkpoint(self, filename: str = "checkpoint.pt"):
        """Load model checkpoint."""
        if self.checkpoint_dir is None:
            return

        checkpoint_path = self.checkpoint_dir / filename
        if not checkpoint_path.exists():
            print(f"Checkpoint {checkpoint_path} not found")
            return

        checkpoint = torch.load(checkpoint_path, weights_only=False)

        self.model.load_state_dict(checkpoint["model_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        self.current_epoch = checkpoint["epoch"]
        self.history = checkpoint["history"]

        print(f"Checkpoint loaded from {checkpoint_path}")

    def train(self):
        """
        Main training loop.

        Customize with:
        - Early stopping
        - Learning rate scheduling
        - Gradient clipping
        - Custom callbacks
        """
        print(f"Starting training for {self.max_epochs} epochs")

        for epoch in range(self.max_epochs):
            self.current_epoch = epoch

            # Train
            train_loss = self.train_epoch()
            self.history["train_loss"].append(train_loss)

            # Validate
            val_loss = self.validate()
            if val_loss is not None:
                self.history["val_loss"].append(val_loss)

            # Print progress
            val_str = f", Val Loss: {val_loss:.6f}" if val_loss else ""
            print(f"Epoch {epoch+1}/{self.max_epochs} - Train Loss: {train_loss:.6f}{val_str}")

            # Save checkpoint periodically
            if (epoch + 1) % 10 == 0 and self.checkpoint_dir:
                self.save_checkpoint(f"checkpoint_epoch_{epoch+1}.pt")

        print("Training complete!")

        # Save final checkpoint
        if self.checkpoint_dir:
            self.save_checkpoint("final_checkpoint.pt")


# ============================================================================
# Example Usage
# ============================================================================

if __name__ == "__main__":
    # Example: Simple regression problem

    # 1. Create dummy data
    train_data = torch.utils.data.TensorDataset(
        torch.randn(1000, 10),  # inputs
        torch.randn(1000, 1)    # targets
    )
    train_loader = torch.utils.data.DataLoader(train_data, batch_size=32)

    # 2. Create model
    model = torch.nn.Sequential(
        torch.nn.Linear(10, 64),
        torch.nn.ReLU(),
        torch.nn.Linear(64, 1)
    )

    # 3. Create optimizer and loss
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    loss_fn = torch.nn.MSELoss()

    # 4. Create trainer
    trainer = GenericTrainer(
        model=model,
        train_loader=train_loader,
        optimizer=optimizer,
        loss_fn=loss_fn,
        device="cpu",
        max_epochs=50,
        checkpoint_dir=Path("checkpoints")
    )

    # 5. Train
    trainer.train()

    # 6. Access history
    print(f"Final train loss: {trainer.history['train_loss'][-1]:.6f}")
